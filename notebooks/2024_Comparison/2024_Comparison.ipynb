{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Knowledge Graphs at Production Scale\n",
    "Using Knowledge Graphs to improve the results of Retrieval-Augmented Generation (RAG) applications is widely discussed. Most examples demonstrate how to build a knowledge graph using a relatively small number of documents. This may be because the typical approach – extracting fine-grained, entity-centric information just doesn’t scale. Running each document through a model to extract the entities (nodes) and relationships (edges) takes too long (and costs too much) to run on large datasets.\n",
    "\n",
    "We’ve talked about the idea of content-centric knowledge graphs – a vector-store allowing links between chunks – as an easier to use and more efficient approach. In this post we put that to the test. We load a subset of the wikipedia articles from the [2wikimultihop](https://github.com/Alab-NII/2wikimultihop) dataset using both techniques and discuss what this means for loading the entire dataset. We demonstrate the results of some questions over the loaded data. We’ll also load the entire dataset – nearly 6 million documents – into a content-centric [GraphVectorStore](https://www.datastax.com/blog/now-in-langchain-graph-vector-store-add-structured-data-to-rag-apps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Install modules\n",
    "%pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Configure import paths.\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Initialize environment variables.\n",
    "from utils import initialize_environment\n",
    "initialize_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to Load\n",
    "For this notebook, we'll work on loading the first 100 articles from Wikipedia. We use Wikipedia data from the [2wikimultihop](https://github.com/Alab-NII/2wikimultihop) dataset. To execute the rest of the notebook, you will need to download [para_with_hyperlink.zip](https://www.dropbox.com/s/wlhw26kik59wbh8/para_with_hyperlink.zip) to the `wikimultihop` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../../datasets/wikimultihop/para_with_hyperlink.zip' already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "from utils import download_file\n",
    "download_file(\"https://www.dropbox.com/s/wlhw26kik59wbh8/para_with_hyperlink.zip?dl=1\", \"../../datasets/wikimultihop/para_with_hyperlink.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from datasets.wikimultihop.load import wikipedia_lines\n",
    "\n",
    "NUM_LINES_TO_LOAD = 100\n",
    "lines_to_load = list(islice(wikipedia_lines(), NUM_LINES_TO_LOAD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Centric: LLMGraphTrasnformer\n",
    "\n",
    "Loading documents into an entity-centric graph store like Neo4j was done using LangChain’s `LLMGraphTransformer`. The code is based on LangChain's [\"How to construct knowledge graphs\"](https://python.langchain.com/docs/how_to/graph_constructing/#llm-graph-transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded (but NOT written) 100 in 477.67s\n",
      "OpenAI stats: prompt tokens 116180, completion tokens 26145, total cost 0.0\n"
     ]
    }
   ],
   "source": [
    "#@ Extract GraphDocuments\n",
    "from langchain_core.documents import Document\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.callbacks import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "from time import perf_counter\n",
    "start = perf_counter()\n",
    "\n",
    "documents_to_load = [Document(page_content=line) for line in lines_to_load]\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    graph_documents = llm_transformer.convert_to_graph_documents(documents_to_load)\n",
    "    end = perf_counter()\n",
    "\n",
    "    print(f\"Loaded (but NOT written) {NUM_LINES_TO_LOAD} in {end - start:0.2f}s\")\n",
    "    print(f\"OpenAI stats: prompt tokens {cb.prompt_tokens}, completion tokens {cb.completion_tokens}, total cost {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a Neo4j Docker instance:\n",
    "\n",
    "### Unix\n",
    "```bash\n",
    "docker run -d -p=7474:7474 -p=7687:7687 -e NEO4J_AUTH=neo4j/password -e NEO4J_PLUGINS=\\[\\\"apoc\\\"\\] neo4j\n",
    "```\n",
    "\n",
    "### Powershell\n",
    "```bash\n",
    "docker run -d -p=7474:7474 -p=7687:7687 -e NEO4J_AUTH=neo4j/password -e NEO4J_PLUGINS='\"[\\\"apoc\\\"]\"' neo4j\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written in 12.77s\n"
     ]
    }
   ],
   "source": [
    "#@ Write GraphDocuments to Neo4j\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "from time import perf_counter\n",
    "start = perf_counter()\n",
    "\n",
    "entity_centric_store = Neo4jGraph(url=\"bolt://localhost:7687\", username=\"neo4j\", password=\"password\")\n",
    "entity_centric_store.add_graph_documents(graph_documents)\n",
    "\n",
    "end = perf_counter()\n",
    "print(f\"Written in {end - start:0.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Centric: GraphVectorStore\n",
    "Loading the data into `GraphVectorStore` is roughly the same as loading it into a vector store. The only addition is that we compute metadata indicating how the pages link to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Configure Tables\n",
    "import cassio\n",
    "cassio.init(auto=True)\n",
    "TABLE_NAME = \"wiki_load\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Empty the table (optional)\n",
    "if input(\"clear data(y/N): \").lower() == \"y\":\n",
    "    print(\"Clearing data...\")\n",
    "    from cassio.config import check_resolve_session, check_resolve_keyspace\n",
    "    session = check_resolve_session()\n",
    "    keyspace = check_resolve_keyspace()\n",
    "\n",
    "    session.execute(f\"TRUNCATE TABLE {keyspace}.{TABLE_NAME};\")\n",
    "    print(\"Done\")\n",
    "else:\n",
    "    print(\"Skipped clearing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Create GraphVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.graph_vectorstores.cassandra import CassandraGraphVectorStore\n",
    "\n",
    "content_centric_store = CassandraGraphVectorStore(\n",
    "    embedding = OpenAIEmbeddings(),\n",
    "    node_table=TABLE_NAME,\n",
    "    #insert_timeout = 1000.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ Add links to documents\n",
    "import json\n",
    "from langchain_core.graph_vectorstores.links import METADATA_LINKS_KEY, Link\n",
    "\n",
    "def parse_document(line: str) -> Document:\n",
    "    para = json.loads(line)\n",
    "\n",
    "    id = para[\"id\"]\n",
    "    links = {\n",
    "        Link.outgoing(kind=\"href\", tag=id)\n",
    "        for m in para[\"mentions\"]\n",
    "        if m[\"ref_ids\"] is not None\n",
    "        for id in m[\"ref_ids\"]\n",
    "    }\n",
    "    links.add(Link.incoming(kind=\"href\", tag=id))\n",
    "    return Document(\n",
    "        id = id,\n",
    "        page_content = \" \".join(para[\"sentences\"]),\n",
    "        metadata = {\n",
    "            \"content_id\": para[\"id\"],\n",
    "            METADATA_LINKS_KEY: list(links)\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entity-centric data...\n",
      "Loaded (and written) 100 in 1.89s\n"
     ]
    }
   ],
   "source": [
    "#@ Load Data Into GraphVectorStore\n",
    "print(\"Loading entity-centric data...\")\n",
    "from time import perf_counter\n",
    "\n",
    "start = perf_counter()\n",
    "kg_documents = [parse_document(line) for line in lines_to_load]\n",
    "content_centric_store.add_documents(kg_documents)\n",
    "end = perf_counter()\n",
    "print(f\"Loaded (and written) {NUM_LINES_TO_LOAD} in {end - start:0.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Benchmarks\n",
    "Running at 100 rows, the entity-centric approach using gpt-4o took 405.93s to extract the GraphDocumuents and 10.99s to write them to Neo4j, while the content-centric approach took 1.43s. Extrapolating, it would take 41 weeks to load all 5,989,847 pages using the entity-centric approach, and about 24-hours using the content-centric approach. However, thanks to parallelism the content-centric approach runs in only 2.5 hours! Assuming the same parallelism benefits, it would still take over 4 weeks to load everything using the entity-centric approach. I didn’t try it, since the estimated cost would be $58,700 assuming everything worked the first time!\n",
    "\n",
    "**Bottom-line, the entity-centric approach of extracting knowledge graphs from content using an LLM was both time and cost prohibitive at scale. On the other hand, using GraphVectorStore was fast and cheap.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phil\\git\\graph-vectorstore-examples\\.venv\\Lib\\site-packages\\langchain\\hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    }
   ],
   "source": [
    "#@ VectorGraphStore RAG chain\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "content_centric_retriever = content_centric_store.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "content_centric_chain = (\n",
    "    {\"context\": content_centric_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import GraphCypherQAChain\n",
    "entity_centric_chain = GraphCypherQAChain.from_llm(graph=entity_centric_store, llm=llm, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Centric\n",
      "--------------\n",
      "Question 1: When was 'The Circle' released?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: title)} {position: line: 2, column: 17, offset: 23} for query: \"cypher\\nMATCH (m:Movie {title: 'The Circle'}) RETURN m.releaseDate\\n\"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownLabelWarning} {category: UNRECOGNIZED} {title: The provided label is not in the database.} {description: One of the labels in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing label name is: Movie)} {position: line: 2, column: 10, offset: 16} for query: \"cypher\\nMATCH (m:Movie {title: 'The Circle'}) RETURN m.releaseDate\\n\"\n",
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: releaseDate)} {position: line: 2, column: 48, offset: 54} for query: \"cypher\\nMATCH (m:Movie {title: 'The Circle'}) RETURN m.releaseDate\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': \"When was 'The Circle' released?\", 'result': \"I don't know the answer.\"}\n",
      "\n",
      "Question 2: Where is Urup located?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.UnknownPropertyKeyWarning} {category: UNRECOGNIZED} {title: The provided property key is not in the database} {description: One of the property names in your query is not available in the database, make sure you didn't misspell it or that the label is available when you run this statement in your application (the missing property name is: name)} {position: line: 2, column: 11, offset: 17} for query: \"cypher\\nMATCH (n {name: 'Urup'})-[:LOCATED_IN]->(location)\\nRETURN location\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Where is Urup located?', 'result': \"I don't know the answer.\"}\n",
      "Entity Centric Time in 5.36s\n",
      "OpenAI stats: prompt tokens 582, completion tokens 61, total cost 0.0\n",
      "\n",
      "Content Centric\n",
      "---------------\n",
      "Question 1: When was 'The Circle' released?\n",
      "The Circle was released in 1988.\n",
      "\n",
      "Question 2: Where is Urup located?\n",
      "Urup is located in Badakhshan Province in north-eastern Afghanistan.\n",
      "Content Centric Time in 1.96s\n",
      "OpenAI stats: prompt tokens 450, completion tokens 24, total cost 0.0\n"
     ]
    }
   ],
   "source": [
    "QUESTION1 = \"When was 'The Circle' released?\"\n",
    "QUESTION2 = \"Where is Urup located?\"\n",
    "\n",
    "print(\"Entity Centric\\n--------------\")\n",
    "start = perf_counter()\n",
    "with get_openai_callback() as cb:\n",
    "    print(f\"Question 1: {QUESTION1}\")\n",
    "    print(entity_centric_chain.invoke(QUESTION1))\n",
    "    print(f\"\\nQuestion 2: {QUESTION2}\")\n",
    "    print(entity_centric_chain.invoke(QUESTION2))\n",
    "\n",
    "    end = perf_counter()\n",
    "    print(f\"Entity Centric Time in {end - start:0.2f}s\")\n",
    "    print(f\"OpenAI stats: prompt tokens {cb.prompt_tokens}, completion tokens {cb.completion_tokens}, total cost {cb.total_cost}\")\n",
    "\n",
    "print(\"\\nContent Centric\\n---------------\")\n",
    "start = perf_counter()\n",
    "with get_openai_callback() as cb:\n",
    "    print(f\"Question 1: {QUESTION1}\")\n",
    "    print(content_centric_chain.invoke(QUESTION1))\n",
    "    print(f\"\\nQuestion 2: {QUESTION2}\")\n",
    "    print(content_centric_chain.invoke(QUESTION2))\n",
    "\n",
    "    end = perf_counter()\n",
    "    print(f\"Content Centric Time in {end - start:0.2f}s\")\n",
    "    print(f\"OpenAI stats: prompt tokens {cb.prompt_tokens}, completion tokens {cb.completion_tokens}, total cost {cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be surprising that the fine-grained Neo4j graph returns useless answers. Looking at the logging from the chain, we see some of why this happens:\n",
    "\n",
    "```\n",
    "> Entering new GraphCypherQAChain chain...\n",
    "Generated Cypher:\n",
    "cypher\n",
    "MATCH (a:Album {id: 'The Circle'})-[:RELEASED_BY]->(r:Record_label)\n",
    "RETURN a.id, r.id\n",
    "\n",
    "Full Context:\n",
    "[{'a.id': 'The Circle', 'r.id': 'Restless'}]\n",
    "\n",
    "> Finished chain.\n",
    "{'query': \"When was 'The Circle' released?\", 'result': \"I don't know the answer.\"}\n",
    "```\n",
    "\n",
    "So, the fine-grained schema only returned information about the record label, which wasn't helpful for answering the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Extracting fine-grained, entity-specific knowledge graphs is time and cost prohibitive at scale. When asked questions over the subset of data that was loaded, the additional granularity (and extra cost loading the fine-grained graph) returned more tokens to include the prompt, but generated useless answers!\n",
    "\n",
    "`GraphVectorStore` takes a coarse-grained, content-centric approach that makes it fast and easy to build a knowledge graph. You can start with your existing code for populating a `VectorStore` using LangChain and add links (edges) between chunks to improve the retrieval process.\n",
    "\n",
    "Graph RAG is a useful tool for enabling GenAI RAG applications to retrieve more deeply relevant context. But using a fine-grained, entity-centric approach does not scale to production needs. If you're looking to add knowledge graph capabilities to your RAG application, try `GraphVectorStore`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
